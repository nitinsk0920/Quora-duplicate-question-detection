{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13426289,"sourceType":"datasetVersion","datasetId":8521780},{"sourceId":13432498,"sourceType":"datasetVersion","datasetId":8525732},{"sourceId":13434814,"sourceType":"datasetVersion","datasetId":8527342},{"sourceId":13441174,"sourceType":"datasetVersion","datasetId":8531455},{"sourceId":13441575,"sourceType":"datasetVersion","datasetId":8531757}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# ===============================================================\n# üì¶ Imports & GPU setup\n# ===============================================================\n!pip install transformers xgboost beautifulsoup4 --quiet\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import DistilBertTokenizer, DistilBertModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nimport xgboost as xgb\nimport re\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm\ntqdm.pandas()\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(\"‚úÖ Using device:\", device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================================================\n# üßπ 1. Text Preprocessing\n# ===============================================================\ndef preprocess_text(q):\n    q = str(q).lower().strip()\n    q = q.replace('%', ' percent').replace('$', ' dollar ').replace('‚Çπ', ' rupee ').replace('‚Ç¨', ' euro ').replace('@', ' at ')\n    q = q.replace('[math]', '')\n    q = q.replace(',000,000,000 ', 'b ').replace(',000,000 ', 'm ').replace(',000 ', 'k ')\n    q = re.sub(r'([0-9]+)000000000', r'\\1b', q)\n    q = re.sub(r'([0-9]+)000000', r'\\1m', q)\n    q = re.sub(r'([0-9]+)000', r'\\1k', q)\n\n    contractions = {\n        \"can't\": \"cannot\", \"won't\": \"will not\", \"i'm\": \"i am\", \"you're\": \"you are\",\n        \"he's\": \"he is\", \"she's\": \"she is\", \"it's\": \"it is\", \"that's\": \"that is\",\n        \"they're\": \"they are\", \"isn't\": \"is not\", \"aren't\": \"are not\", \"wasn't\": \"was not\",\n        \"weren't\": \"were not\", \"haven't\": \"have not\", \"hasn't\": \"has not\", \"didn't\": \"did not\"\n    }\n\n    q = ' '.join([contractions[word] if word in contractions else word for word in q.split()])\n    q = BeautifulSoup(q, 'html.parser').get_text()\n    q = re.sub(r'\\W', ' ', q).strip()\n    return q\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/test-csv/test.csv\")  # adjust path\ndf = df.dropna(subset=['question1', 'question2'])\n\ndf['question1'] = df['question1'].apply(preprocess_text)\ndf['question2'] = df['question2'].apply(preprocess_text)\n\nprint(df.head(2))\nprint(\"‚úÖ Preprocessing done.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ping -c 2 huggingface.co\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nmodel = DistilBertModel.from_pretrained('distilbert-base-uncased')\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\nmodel.eval()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_sentence_embedding(sentence):\n    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True, max_length=64)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    with torch.no_grad():\n        outputs = model(**inputs)\n    # Mean pooling of token embeddings                                                                                                                                                                                                                                                                                                     \n    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"q1_embeddings = np.vstack([get_sentence_embedding(q) for q in df['question1']])\nq2_embeddings = np.vstack([get_sentence_embedding(q) for q in df['question2']])\n\n# Save for reuse\nnp.save(\"/kaggle/working/q1_embeddings.npy\", q1_embeddings)\nnp.save(\"/kaggle/working/q2_embeddings.npy\", q2_embeddings)\nprint(\"‚úÖ Embeddings saved in /kaggle/working/\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" X_pair = np.hstack((q1_embeddings, q2_embeddings, np.abs(q1_embeddings - q2_embeddings))).reshape(1, -1)\n#X = np.abs(q1_embeddings - q2_embeddings)\ny = df['is_duplicate'].values\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom numpy.linalg import norm\nimport os\n\n# -------------------------------\n# 1Ô∏è‚É£ Load DistilBERT Embeddings\n# -------------------------------\nq1_embeddings = np.load(\"/kaggle/input/que1-2-embeds/q1_embeddings.npy\")\nq2_embeddings = np.load(\"/kaggle/input/que1-2-embeds/q2_embeddings.npy\")\n\nprint(\"‚úÖ Q1 embeddings:\", q1_embeddings.shape)\nprint(\"‚úÖ Q2 embeddings:\", q2_embeddings.shape)\n\n# -------------------------------\n# 2Ô∏è‚É£ Feature Engineering\n# -------------------------------\n\n# Absolute difference\nfeat_abs_diff = np.abs(q1_embeddings - q2_embeddings)\n\n# Element-wise product\nfeat_product = q1_embeddings * q2_embeddings\n\n# Cosine similarity (scalar feature per pair)\ncos_sim = np.array([\n    cosine_similarity(q1_embeddings[i].reshape(1, -1),\n                      q2_embeddings[i].reshape(1, -1))[0][0]\n    for i in range(len(q1_embeddings))\n]).reshape(-1, 1)\n\n# Euclidean distance (scalar feature per pair)\neuclid_dist = np.array([\n    norm(q1_embeddings[i] - q2_embeddings[i])\n    for i in range(len(q1_embeddings))\n]).reshape(-1, 1)\n\n# Concatenate both question embeddings\nfeat_concat = np.concatenate([q1_embeddings, q2_embeddings], axis=1)\n\n# -------------------------------\n# 3Ô∏è‚É£ Combine All Features\n# -------------------------------\nX_final = np.concatenate([\n    feat_concat,      # 1536 dims\n    feat_abs_diff,    # 768 dims\n    feat_product,     # 768 dims\n    cos_sim,          # 1 dim\n    euclid_dist       # 1 dim\n], axis=1)\n\nprint(\"‚úÖ Final feature matrix shape:\", X_final.shape)\n\n# -------------------------------\n# 4Ô∏è‚É£ Save for future use\n# -------------------------------\nsave_path = \"/kaggle/working/X_final.npy\"\nnp.save(save_path, X_final)\nprint(f\"üíæ Saved final combined features to: {save_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_final=np.load(\"/kaggle/input/text-feat-embeds/X_final.npy\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Load your original train.csv\ndf = pd.read_csv(\"/kaggle/input/quora-duplicates/train.csv\")\n\n# Drop rows with missing questions ‚Äî must match the preprocessing used for embeddings\ndf = df[['question1', 'question2', 'is_duplicate']].dropna().reset_index(drop=True)\n\n# Ensure same number of samples as embeddings\ny = df['is_duplicate'].values\n\nprint(\"‚úÖ After cleaning:\")\nprint(\"Labels shape:\", y.shape)\nprint(\"X_final shape:\", X_combined.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"min_len = min(len(X_final), len(y))\nX_final = X_final[:min_len]\ny = y[:min_len]\nprint(\"‚úÖ Shapes aligned:\", X_final.shape, y.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/quora-duplicates/train.csv\")  # adjust path\n\ny = df['is_duplicate'].values\ny.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom fuzzywuzzy import fuzz\n\n# -------------------------------\n# Load CSV\n# -------------------------------\ndf = pd.read_csv(\"/kaggle/input/quora-duplicates/train.csv\")\ndf = df[['question1','question2','is_duplicate']].dropna().reset_index(drop=True)\n\n# -------------------------------\n# Handcrafted + Fuzzy Features\n# -------------------------------\ndef extract_features(row):\n    q1 = str(row['question1'])\n    q2 = str(row['question2'])\n    \n    # Length features\n    len_q1 = len(q1)\n    len_q2 = len(q2)\n    char_count_diff = abs(len_q1 - len_q2)\n    \n    # Word count\n    wc_q1 = len(q1.split())\n    wc_q2 = len(q2.split())\n    word_count_diff = abs(wc_q1 - wc_q2)\n    \n    # Common words\n    common_words = len(set(q1.lower().split()) & set(q2.lower().split()))\n    common_word_ratio = common_words / (wc_q1 + wc_q2 + 1e-5)  # avoid div 0\n    \n    # Fuzzy features\n    fuzz_ratio = fuzz.ratio(q1, q2)\n    fuzz_partial_ratio = fuzz.partial_ratio(q1, q2)\n    token_sort_ratio = fuzz.token_sort_ratio(q1, q2)\n    token_set_ratio = fuzz.token_set_ratio(q1, q2)\n    \n    return pd.Series([\n        len_q1, len_q2, char_count_diff,\n        wc_q1, wc_q2, word_count_diff,\n        common_words, common_word_ratio,\n        fuzz_ratio, fuzz_partial_ratio, token_sort_ratio, token_set_ratio\n    ])\n\n# Apply features\nhandcrafted_feats = df.apply(extract_features, axis=1)\nhandcrafted_feats.columns = [\n    'len_q1','len_q2','char_count_diff',\n    'wc_q1','wc_q2','word_count_diff',\n    'common_words','common_word_ratio',\n    'fuzz_ratio','fuzz_partial_ratio','token_sort_ratio','token_set_ratio'\n]\n\nprint(\"‚úÖ Handcrafted features shape:\", handcrafted_feats.shape)\n\n# -------------------------------\n# Combine with existing embeddings features\n# -------------------------------\n# Assuming X_final.npy is your DistilBERT + engineered features (3074 dims)\nX_final = np.load(\"/kaggle/input/text-feat-embeds/X_final.npy\")\n\n# Align lengths\nmin_len = min(len(X_final), len(handcrafted_feats))  \nX_final = X_final[:min_len]\nhandcrafted_feats = handcrafted_feats.iloc[:min_len]\ny = df['is_duplicate'].values[:min_len]\n\n# Combine\nX_combined = np.concatenate([X_final, handcrafted_feats.values], axis=1)\nprint(\"‚úÖ Final combined feature shape:\", X_combined.shape)\n\nimport numpy as np\nsave_path = \"/kaggle/working/X_combined.npy\"\nnp.save(save_path,X_combined )\nprint(f\"üíæ Saved final combined features to: {save_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined=np.load(\"/kaggle/input/combined-embeds/X_combined.npy\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_combined, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(\"‚úÖ Train shape:\", X_train.shape)\nprint(\"‚úÖ Test shape:\", X_test.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_train.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmodel_xgb = xgb.XGBClassifier(\n    n_estimators=500,\n    learning_rate=0.03,\n    max_depth=8,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    objective='binary:logistic',\n    eval_metric='logloss',\n    tree_method='gpu_hist'\n)\nmodel_xgb.fit(X_train, y_train)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\ny_pred = (model_xgb.predict_proba(X_test)[:, 1] > 0.5).astype(int)\n\nprint(\"‚úÖ Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\n\njoblib.dump(model_xgb , \"/kaggle/working/xgb_quora_model_combined.pkl\")\nprint(\"üíæ Model saved successfully at /kaggle/working/xgb_quora_model_combined.pkl\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import DistilBertTokenizer, DistilBertModel\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom numpy.linalg import norm\nimport joblib\n\n# -----------------------------------------------------------\n# 1Ô∏è‚É£ Load saved model and tokenizer\n# -----------------------------------------------------------\nxgb_model = joblib.load(\"/kaggle/working/xgb_quora_model.pkl\")\n\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\nbert_model.to('cuda')\nbert_model.eval()\n\n# -----------------------------------------------------------\n# 2Ô∏è‚É£ Preprocessing function (same as training)\n# -----------------------------------------------------------\nimport re\nfrom bs4 import BeautifulSoup\n\ndef preprocess(q):\n    q = str(q).lower().strip()\n    q = q.replace('%', ' percent').replace('$', ' dollar ').replace('‚Çπ', ' rupee ')\n    q = q.replace('‚Ç¨', ' euro ').replace('@', ' at ')\n    q = BeautifulSoup(q, 'html.parser').get_text()\n    q = re.sub(r'[^a-zA-Z0-9\\s]', ' ', q)\n    q = re.sub(r'\\s+', ' ', q).strip()\n    return q\n\n# -----------------------------------------------------------\n# 3Ô∏è‚É£ Generate DistilBERT embeddings\n# -----------------------------------------------------------\ndef get_bert_embedding(text):\n    tokens = tokenizer(\n        text,\n        padding=True,\n        truncation=True,\n        return_tensors='pt',\n        max_length=64\n    ).to('cuda')\n\n    with torch.no_grad():\n        outputs = bert_model(**tokens)\n        embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()  # mean pooling\n    return embedding\n\n  # Concatenate both question embeddings\n  \n# -----------------------------------------------------------\n# 4Ô∏è‚É£ Feature Engineering for single pair\n# -----------------------------------------------------------\ndef make_features(q1_emb, q2_emb, q1, q2):\n    # Basic numerical features\n    feat_abs_diff = np.abs(q1_emb - q2_emb)\n    feat_product = q1_emb * q2_emb\n\n    # Similarities\n    cos_sim = cosine_similarity(q1_emb, q2_emb)[0][0]\n    euclid_dist = norm(q1_emb - q2_emb)\n\n    # Combine embeddings\n    feat_concat = np.concatenate([q1_emb, q2_emb], axis=1)\n\n    # ‚úÖ Final feature vector\n    X_final = np.concatenate(\n        [\n            feat_concat,              # (1, 1536)\n            feat_abs_diff,            # (1, 768)\n            feat_product,             # (1, 768)               # (1, 12)\n            np.array([[cos_sim]]),    # (1, 1)\n            np.array([[euclid_dist]]) # (1, 1)\n        ],\n        axis=1\n    )\n\n    return X_final\n\n\n# -----------------------------------------------------------\n# 5Ô∏è‚É£ Prediction function\n# -----------------------------------------------------------\ndef predict_duplicate(q1, q2):\n    q1_prep = preprocess(q1)\n    q2_prep = preprocess(q2)\n\n    q1_emb = get_bert_embedding(q1_prep)\n    q2_emb = get_bert_embedding(q2_prep)\n\n    X_input = make_features(q1_emb, q2_emb,q1,q2)\n    \n    pred = xgb_model.predict(X_input)[0]\n    prob = xgb_model.predict_proba(X_input)[0][1]\n\n    label = \"‚úÖ Duplicate\" if pred == 1 else \"‚ùå Not Duplicate\"\n    print(f\"\\nPrediction: {label} (Confidence: {prob:.2f})\")\n\n# -----------------------------------------------------------\n# üî• Example test\n# ----------------------------------------------------------\ngame=True\nwhile game==True:\n    end=input(\"DO YOU WANNA END?,type Y for yes and N for NO\").lower()\n    if end=='y':\n        game=False \n    else:\n        que1=input(\"enter question 1:\")\n        que2=input(\"enter question 2:\")\n        predict_duplicate(que1,que2)\n    \n\n    \n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nimport joblib\n\n\nX_combined=np.load(\"/kaggle/working/X_combined.npy\")\nscaler = StandardScaler()\nX_combined_scaled = scaler.fit_transform(X_combined)\n\n# Save for inference use\njoblib.dump(scaler, \"/kaggle/working/feature_scaler.pkl\")\n\nnp.save(\"/kaggle/working/X_combined_scaled.npy\", X_combined_scaled)\nprint(\"‚úÖ Scaled & saved features + scaler.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import DistilBertTokenizer, DistilBertModel\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom numpy.linalg import norm\nimport joblib\nfrom fuzzywuzzy import fuzz\nimport re\nfrom bs4 import BeautifulSoup\n\n# -------------------------------\n# Load saved XGBoost model and tokenizer\n# -------------------------------\nxgb_model = joblib.load(\"/kaggle/working/xgb_quora_model2.pkl\")\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\nbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased').to('cuda')\nbert_model.eval()\n\n# -------------------------------\n# Preprocessing function\n# -------------------------------\ndef preprocess(q):\n    q = str(q).lower().strip()\n    q = q.replace('%',' percent').replace('$',' dollar ').replace('‚Çπ',' rupee ')\n    q = q.replace('‚Ç¨',' euro ').replace('@',' at ')\n    q = BeautifulSoup(q,'html.parser').get_text()\n    q = re.sub(r'[^a-zA-Z0-9\\s]',' ',q)\n    q = re.sub(r'\\s+',' ',q).strip()\n    return q\n\n# -------------------------------\n# Generate BERT embedding\n# -------------------------------\ndef get_bert_embedding(text):\n    tokens = tokenizer(text, padding=True, truncation=True, return_tensors='pt', max_length=64).to('cuda')\n    with torch.no_grad():\n        outputs = bert_model(**tokens)\n        embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n    return embedding\n\n# -------------------------------\n# Handcrafted + Fuzzy Features\n# -------------------------------\ndef extract_features(q1, q2):\n    len_q1, len_q2 = len(q1), len(q2)\n    char_diff = abs(len_q1 - len_q2)\n    wc_q1, wc_q2 = len(q1.split()), len(q2.split())\n    word_diff = abs(wc_q1 - wc_q2)\n    common_words = len(set(q1.lower().split()) & set(q2.lower().split()))\n    common_ratio = common_words / (wc_q1 + wc_q2 + 1e-5)\n    \n    fuzz_ratio = fuzz.ratio(q1, q2)\n    fuzz_partial = fuzz.partial_ratio(q1, q2)\n    token_sort = fuzz.token_sort_ratio(q1, q2)\n    token_set = fuzz.token_set_ratio(q1, q2)\n    \n    return np.array([[len_q1, len_q2, char_diff,\n                      wc_q1, wc_q2, word_diff,\n                      common_words, common_ratio,\n                      fuzz_ratio, fuzz_partial, token_sort, token_set]])\n\n# -------------------------------\n# Create features exactly as training\n# -------------------------------\ndef make_features(q1, q2):\n    # Preprocess\n    q1_p, q2_p = preprocess(q1), preprocess(q2)\n    # BERT embeddings\n    q1_emb, q2_emb = get_bert_embedding(q1_p), get_bert_embedding(q2_p)\n    \n    # Training pipeline features\n    feat_concat = np.concatenate([q1_emb, q2_emb], axis=1)\n    feat_abs_diff = np.abs(q1_emb - q2_emb)\n    feat_product = q1_emb * q2_emb\n    cos_sim = np.array([[cosine_similarity(q1_emb, q2_emb)[0][0]]])\n    euclid_dist = np.array([[norm(q1_emb - q2_emb)]])\n    \n    # Handcrafted + fuzzy\n    hand_feat = extract_features(q1, q2)\n    \n    # Concatenate all\n    X_input = np.concatenate([feat_concat, feat_abs_diff, feat_product, cos_sim, euclid_dist, hand_feat], axis=1)\n    return X_input\n\n# -----------------------------------------------------------\n# 6Ô∏è‚É£ Prediction Function with adjustable threshold\n# -----------------------------------------------------------\ndef predict_duplicate(q1, q2, threshold=0.48):\n    # Preprocess\n    q1_prep = preprocess(q1)\n    q2_prep = preprocess(q2)\n\n    # Get embeddings\n    q1_emb = get_bert_embedding(q1_prep)\n    q2_emb = get_bert_embedding(q2_prep)\n\n    # Create feature vector\n    X_input = make_features(q1_emb, q2_emb)\n\n    # Predict probability\n    prob = xgb_model.predict_proba(X_input)[0][1]\n\n    # Apply threshold\n    label = \"‚úÖ Duplicate\" if prob >= threshold else \"‚ùå Not Duplicate\"\n\n    # Display\n    print(f\"\\nPrediction: {label} (Confidence: {prob:.2f}, Threshold: {threshold})\")\n\n# -------------------------------\n# Interactive Testing\n# -------------------------------\nwhile True:\n    end = input(\"DO YOU WANNA END? Type Y for yes and N for NO: \").lower()\n    if end=='y':\n        break\n    q1 = input(\"Enter Question 1: \")\n    q2 = input(\"Enter Question 2: \")\n    predict_duplicate(q1, q2)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"min_len = min(len(X_combined), len(handcrafted_feats))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def make_features(q1_emb, q2_emb,q1,q2):\n    # Absolute difference\n    feat_abs_diff = np.abs(q1_embeddings - q2_embeddings)\n    \n    # Element-wise product\n    feat_product = q1_embeddings * q2_embeddings\n    \n    # Cosine similarity (scalar feature per pair)\n    cos_sim = np.array([\n        cosine_similarity(q1_embeddings[i].reshape(1, -1),\n                          q2_embeddings[i].reshape(1, -1))[0][0]\n        for i in range(len(q1_embeddings))\n    ]).reshape(-1, 1)\n    \n    # Euclidean distance (scalar feature per pair)\n    euclid_dist = np.array([\n        norm(q1_embeddings[i] - q2_embeddings[i])\n        for i in range(len(q1_embeddings))\n    ]).reshape(-1, 1)\n    # -------------------------------\n    # 3Ô∏è‚É£ Combine All Features\n    # -------------------------------\n    X_final = np.concatenate([\n        feat_concat,      # 1536 dims\n        feat_abs_diff,    # 768 dims\n        feat_product,     # 768 dims\n        cos_sim,          # 1 dim\n        euclid_dist,\n        hand_craft.values# 1 dim\n    ], axis=1)\n    \n    return X_final","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom fuzzywuzzy import fuzz","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------\n# Handcrafted + Fuzzy Features\n# -------------------------------\ndef extract_features(que1,que2):\n    q1 = que1\n    q2 = que2\n    \n    # Length features\n    len_q1 = len(q1)\n    len_q2 = len(q2)\n    char_count_diff = abs(len_q1 - len_q2)\n    \n    # Word count\n    wc_q1 = len(q1.split())\n    wc_q2 = len(q2.split())\n    word_count_diff = abs(wc_q1 - wc_q2)\n    \n    # Common words\n    common_words = len(set(q1.lower().split()) & set(q2.lower().split()))\n    common_word_ratio = common_words / (wc_q1 + wc_q2 + 1e-5)  # avoid div 0\n    \n    # Fuzzy features\n    fuzz_ratio = fuzz.ratio(q1, q2)\n    fuzz_partial_ratio = fuzz.partial_ratio(q1, q2)\n    token_sort_ratio = fuzz.token_sort_ratio(q1, q2)\n    token_set_ratio = fuzz.token_set_ratio(q1, q2)\n    \n    return pd.Series([\n        len_q1, len_q2, char_count_diff,\n        wc_q1, wc_q2, word_count_diff,\n        common_words, common_word_ratio,\n        fuzz_ratio, fuzz_partial_ratio, token_sort_ratio, token_set_ratio\n    ])\n\n# Apply features\n\n# -------------------------------","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}